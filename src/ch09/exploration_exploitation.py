import matplotlib.pyplot as plt
import numpy as np


class Bandit:
    def __init__(self, n_actions=5):
        self._probs = np.random.random(n_actions)
        self.n_actions = n_actions

    def pull(self, action):
        if np.random.random() > self._probs[action]:
            return 0.0
        return 1.0

    def optimal_reward(self):
        return np.max(self._probs)


class RandomAgent:
    def __init__(self, n_actions=5):
        self.success_cnt = np.zeros(n_actions)
        self.failure_cnt = np.zeros(n_actions)
        self.total_pulls = 0
        self.n_actions = n_actions

    def reset(self):
        self.success_cnt = np.zeros(self.n_actions)
        self.failure_cnt = np.zeros(self.n_actions)
        self.total_pulls = 0

    def get_action(self):
        return np.random.randint(0, self.n_actions)

    def update(self, action, reward):
        self.total_pulls += 1
        self.success_cnt[action] += reward
        self.failure_cnt[action] += 1 - reward


class EGreedyAgent(RandomAgent):
    def __init__(self, n_actions=5, epsilon=0.01):
        super().__init__(n_actions)
        self.epsilon = epsilon

    def get_action(self):
        estimates = self.success_cnt / (self.success_cnt + self.failure_cnt + 1e-12)
        if np.random.random() < self.epsilon:
            return np.random.randint(0, self.n_actions)
        else:
            return np.argmax(estimates)


class UCBAgent(RandomAgent):
    def get_action(self):
        exploit = self.success_cnt / (self.success_cnt + self.failure_cnt + 1e-12)
        explore = np.sqrt(
            2
            * np.log(np.maximum(self.total_pulls, 1))
            / (self.success_cnt + self.failure_cnt + 1e-12)
        )
        estimates = exploit + explore
        return np.argmax(estimates)


class ThompsonAgent(RandomAgent):
    def get_action(self):
        estimates = np.random.beta(self.success_cnt + 1, self.failure_cnt + 1)
        return np.argmax(estimates)


def get_regret(env, agent, n_steps=10000, n_trials=10):
    score = np.zeros(n_steps)
    optimal_r = env.optimal_reward()

    for trial in range(n_trials):
        agent.reset()
        for t in range(n_steps):
            action = agent.get_action()
            reward = env.pull(action)
            agent.update(action, reward)
            score[t] += optimal_r - reward
    score = score / n_trials
    score = np.cumsum(score)
    return score


def plot_regret(agents, scores):
    for k, v in agents.items():
        plt.plot(scores[k])
    plt.legend([k for k, v in agents.items()])
    plt.ylabel("regret")
    plt.xlabel("steps")
    plt.show(block=False)
    plt.pause(3)
    plt.close()


def main():
    n_actions = 5
    epsilon = 0.01

    agents = {
        # "Random" : RandomAgent(n_actions),
        f"EpsilonGreedy(eps={epsilon})": EGreedyAgent(n_actions, epsilon),
        "UCB": UCBAgent(n_actions),
        "Thompson": ThompsonAgent(n_actions),
    }

    env = Bandit(n_actions)
    scores = {}

    for name, agent in agents.items():
        score = get_regret(env, agent)
        scores[name] = score

    plot_regret(agents, scores)


if __name__ == "__main__":
    main()
