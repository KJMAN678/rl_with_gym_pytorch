```sh
### 2章 マルコフ決定課程
# Gym サンプル MountainCar-v0
python src/ch02/sample_gym.py

# マルコフ決定のサンプルコード
python src/ch02/marcov.py


### 3章 動的計画法
python src/ch03/dp01.py

python src/ch03/dp02.py
```

- 結論
- 最適な状態値は、最も近い終端状態に到達するのに必要なステップ数の負の値であることがわかる。
  エージェントが終末状態に到達するまでの各時間ステップで報酬が-1 されるため、最適な方策は最小限のステップ数でエージェントを終末状態に導くことになる。
- いくつかの状態では、複数のアクションが同じステップ数で終端状態に到達する可能性があります。
  例えば、右上の状態（値-3）を見てみると、左上の終端状態にも右下の終端状態にも 3 ステップで到達することができます。
- つまり、状態の値は、その状態と最も近い終端状態との間のマンハッタン距離の負値である。

```sh
python src/ch03/dp03.py
```

- 結論
- sample03.py の Policy Iteration と同様に、最適な状態値は、最も近いターミナル状態に必要なステップ数の負の値であることがわかります。
- 報酬は、エージェントが終端状態に到達するまでの各時間ステップに対して-1 であるため、最適なポリシーは、最小の可能なステップ数でエージェントを終端状態に連れて行くでしょう。
- いくつかの状態では、複数のアクションが同じステップ数でターミナル状態に到達する可能性があります。
- 例えば、右上の状態を value=-3 とすると、左上の終端状態にも右下の終端状態にも 3 ステップで到達することになる。
- つまり、状態の値は、その状態と最も近い終端状態との間のマンハッタン距離の負値である。

# ４章 モデルフリー

```sh

```
