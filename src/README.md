```sh
### 2章 マルコフ決定課程
# Gym サンプル MountainCar-v0
python3 src/ch02/sample_gym.py

# マルコフ決定のサンプルコード
python3 src/ch02/marcov.py


### 3章 動的計画法
python3 src/ch03/dp01.py

python3 src/ch03/dp02.py
```

- 結論
- 最適な状態値は、最も近い終端状態に到達するのに必要なステップ数の負の値であることがわかる。
  エージェントが終末状態に到達するまでの各時間ステップで報酬が-1 されるため、最適な方策は最小限のステップ数でエージェントを終末状態に導くことになる。
- いくつかの状態では、複数のアクションが同じステップ数で終端状態に到達する可能性があります。
  例えば、右上の状態（値-3）を見てみると、左上の終端状態にも右下の終端状態にも 3 ステップで到達することができます。
- つまり、状態の値は、その状態と最も近い終端状態との間のマンハッタン距離の負値である。

```sh
python3 src/ch03/dp03.py
```

- 結論
- sample03.py の Policy Iteration と同様に、最適な状態値は、最も近いターミナル状態に必要なステップ数の負の値であることがわかります。
- 報酬は、エージェントが終端状態に到達するまでの各時間ステップに対して-1 であるため、最適なポリシーは、最小の可能なステップ数でエージェントを終端状態に連れて行くでしょう。
- いくつかの状態では、複数のアクションが同じステップ数でターミナル状態に到達する可能性があります。
- 例えば、右上の状態を value=-3 とすると、左上の終端状態にも右下の終端状態にも 3 ステップで到達することになる。
- つまり、状態の値は、その状態と最も近い終端状態との間のマンハッタン距離の負値である。

# ４章 モデルフリー

```sh
python3 src/ch04/mc.py
```

- 結論
- 100 エピソードのシミュレーションでは、状態の値があまり収束していないことがわかります。
- しかし、10,000 エピソードのシミュレーションでは、DP による政策評価を行った list3_2 で見た値と一致し、非常によく収束しています。

```sh
python3 src/ch04/glie.py
```

- 結論
- 1000 回のシミュレーションで、政策と状態の値はかなりよく収束していることがわかります。
- しかし、イプシロンの減衰が速いためか、誤った値を持つ状態もある。
- ε の値によって制御される初期エピソードでは、より多くの探索が必要である。
- 1/k としていますが、これは探索を非常に早く減少させます。
- 最初の 100-1000 エピソードは ε=0.05 とかにして、その後 ε を 1/k にするのがいいかもしれません。

```sh
python3 src/ch04/sarsa.py
```

- 結論
- SARSA エージェントは、500 回の学習が終了するまでに最適な方針を学習することがわかる。
- 学習した方針は、崖を避けるために、まず上まで行き、それから右折してゴールに向かって歩くというものである。
- これは、エージェントが崖を乗り越えてゴールに到達する方針を学習すると予想されたからで、エージェントが学習した方針と比較して 4 歩短い最短経路となる。
- しかし、ε-greedy を用いた探索を続けているため、エージェントが崖に近づいたときに、ランダムな行動をとって崖に落ちてしまう可能性が常に少なからずあります。これは、環境について十分に学習した後でも探索を続けるという問題、すなわち、同じ ε-greedy 政策が改善のためのサンプリングにも用いられるという問題を示しています。

```sh
python3 src/ch04/q_learning.py
```

- 結論
- Q エージェントは、Cliff World の場合、500 回の学習で最適な方針を学習することがわかります。
- 学習した方針は探索を伴わないため、エージェントは迷路を横切り、崖のすぐ上の列を右に歩くという最短経路を学習する。
- また、探索がなく行動が決定論的であるため、エージェントは崖の隣のセルで RIGHT 行動をとったときに崖に落ちる可能性はない。
- したがって、エージェントはゴールに向かう最短経路を学習する。
