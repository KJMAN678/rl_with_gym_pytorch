# ４章 モデルフリー

```sh
python3 src/ch04/mc.py
```

- 結論
- 100 エピソードのシミュレーションでは、状態の値があまり収束していないことがわかります。
- しかし、10,000 エピソードのシミュレーションでは、DP による政策評価を行った list3_2 で見た値と一致し、非常によく収束しています。

```sh
python3 src/ch04/glie.py
```

- 結論
- 1000 回のシミュレーションで、政策と状態の値はかなりよく収束していることがわかります。
- しかし、イプシロンの減衰が速いためか、誤った値を持つ状態もある。
- ε の値によって制御される初期エピソードでは、より多くの探索が必要である。
- 1/k としていますが、これは探索を非常に早く減少させます。
- 最初の 100-1000 エピソードは ε=0.05 とかにして、その後 ε を 1/k にするのがいいかもしれません。

```sh
python3 src/ch04/sarsa.py
```

- 結論
- SARSA エージェントは、500 回の学習が終了するまでに最適な方針を学習することがわかる。
- 学習した方針は、崖を避けるために、まず上まで行き、それから右折してゴールに向かって歩くというものである。
- これは、エージェントが崖を乗り越えてゴールに到達する方針を学習すると予想されたからで、エージェントが学習した方針と比較して 4 歩短い最短経路となる。
- しかし、ε-greedy を用いた探索を続けているため、エージェントが崖に近づいたときに、ランダムな行動をとって崖に落ちてしまう可能性が常に少なからずあります。これは、環境について十分に学習した後でも探索を続けるという問題、すなわち、同じ ε-greedy 政策が改善のためのサンプリングにも用いられるという問題を示しています。

```sh
python3 src/ch04/q_learning.py
```

- 結論
- Q エージェントは、Cliff World の場合、500 回の学習で最適な方針を学習することがわかります。
- 学習した方針は探索を伴わないため、エージェントは迷路を横切り、崖のすぐ上の列を右に歩くという最短経路を学習する。
- また、探索がなく行動が決定論的であるため、エージェントは崖の隣のセルで RIGHT 行動をとったときに崖に落ちる可能性はない。
- したがって、エージェントはゴールに向かう最短経路を学習する。

```sh
python3 src/ch04/expected_sarsa.py
```

- 結論
- CLiff World の場合、Expected SARSA Agent は約 1000 エピソードの学習で最適な政策を学習することがわかります。
- 更新式を見ると、Q(S',A')が平均化されていることがわかります。
- つまり、探索をしても、通常の SARA よりも良い方針を傾けることができるのです。
- ここでエージェントが学習した方針は、SARSA と Q-learning の両方を組み合わせたような、真ん中の列を通過するものです。

```sh
python3 src/ch04/replay_buffer.py
```

- 結論
- 再生バッファを持つ Q エージェントは、バッファから繰り返しサンプリングすることで、初期収束性を向上させるとされている。
- サンプルの効率は、DQN を見るとより明らかになります。
- 長期的に見れば、リプレイバッファの有無にかかわらず、学習した最適値に大きな差は生じないでしょう。
- また、サンプル間の相関を断ち切るという利点もあります。
- この点は、Q-Learning を用いた Deep Learning、すなわち DQN を見たときにも明らかになるでしょう。

```sh
python3 src/ch04/q_learning_continuous.py
```

- 結論
- 状態の離散化を行った Q エージェントは、最大報酬 200 に対して、50 程度の報酬を得ることができる。この後の章では、より高い報酬を得るために、より強力な方法を研究します。
